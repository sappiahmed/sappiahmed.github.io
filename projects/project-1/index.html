<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JH41Q4GLYY"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-JH41Q4GLYY');
</script>

  <title>Real-World ML on Wearable Sensor Signals ‚Äî Project | Safi Ahmed</title>
  <meta name="description" content="From raw oscilloscope CSVs to trained models: full pipeline with segmentation, dataset building, training, and evaluation on microcone sliding signals." />

  <!-- Use your global site CSS/JS so design stays consistent -->
  <link rel="stylesheet" href="/css/style.css" />
  <script defer src="/js/script.js"></script>

  <!-- Page-local helpers (same tokens/patterns you‚Äôre using elsewhere) -->
  <style>
    :root{
      --section-gap:28px;
    }

    /* Cards + consistent spacing */
    .section-card{
      background:var(--card); border:1px solid var(--ring); border-radius:var(--radius);
      box-shadow:var(--elev); padding:28px; margin-top:18px;
      transition: background-color .3s, border-color .3s, box-shadow .3s;
      width:100%;
      box-sizing:border-box;
    }

    /* Inline link style to match the main site */
    .section-card a{ color:var(--brand); text-decoration:none; font-weight:500; transition:color .2s,text-decoration .2s; }
    .section-card a:hover{ text-decoration:underline; color:color-mix(in oklab, var(--brand) 80%, var(--text)); }

    /* Pills / small tags */
    .pill{
      display:inline-block; padding:6px 10px; border:1px solid var(--ring);
      border-radius:999px; color:var(--muted); font-size:12px; text-decoration:none;
    }
    .tags{ display:flex; gap:8px; flex-wrap:wrap; margin-top:8px; }
    .tag{ font-size:12px; padding:4px 8px; border-radius:999px; border:1px solid var(--ring);
      background: color-mix(in oklab, var(--brand) 12%, transparent); }

    .small{ font-size:13px; color:var(--muted); }

    /* Two-column helper that never exceeds container width */
    .grid{ display:grid; gap:20px; width:100%; }
    .grid.two{ grid-template-columns: minmax(0,1fr) minmax(0,1fr); }
    .grid.equal{ grid-template-columns: repeat(2, minmax(0,1fr)); }
    @media (max-width:900px){ .grid.two, .grid.equal{ grid-template-columns:1fr; } }

    /* Figure-like layout */
    .figure{ display:grid; column-gap:32px; row-gap:18px; }
    @media (min-width:860px){ .figure{ grid-template-columns:1.2fr 1fr; } }
    .figure img{ width:100%; max-width:600px; height:auto; border-radius:12px; display:block; object-fit:cover; }

    /* Code block */
    .code{
      font-family: ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono",monospace;
      background: color-mix(in oklab, var(--card) 92%, transparent);
      border: 1px dashed var(--ring);
      border-radius: 12px;
      padding: 14px;
      overflow:auto;
      color: var(--text);
    }

    html, body{ overflow-x:hidden; }

    /* Ensure images never overflow section-card */
    .section-card img {
      max-width: 100%;
      height: auto;
      display: block;
      border-radius: 12px;
      margin: 0;
      box-shadow: var(--shadow-sm);
      object-fit: cover;
    }

    /* Make list bullets align with text inside cards */
    .section-card ul,
    .section-card ol {
      padding-left: 1.2rem;
      margin-left: 0;
      list-style-position: inside;
    }

    .section-card li {
      margin: 4px 0;
      line-height: 1.55;
    }

    .actions{
      display:flex;
      gap:10px;
      flex-wrap:wrap;
      margin-top:12px;
    }

    .btn-link{
      display:inline-block;
      padding:8px 12px;
      border:1px solid var(--ring);
      border-radius:10px;
      text-decoration:none;
      color:var(--text);
      background: color-mix(in oklab, var(--card) 80%, transparent);
      box-shadow:var(--shadow-sm);
      transition: transform .15s ease;
    }
    .btn-link:hover{
      transform:translateY(-1px);
    }

    /* Responsive YouTube */
    .video{
      width:100%;
      border-radius:12px;
      overflow:hidden;
      border:1px solid var(--ring);
      box-shadow:var(--shadow-sm);
      background: color-mix(in oklab, var(--card) 92%, transparent);
    }
    .video .ratio{
      position:relative;
      width:100%;
      padding-top:56.25%; /* 16:9 */
    }
    .video iframe{
      position:absolute;
      inset:0;
      width:100%;
      height:100%;
      border:0;
    }

    /* Small divider card for phase headers */
    .phase-header{
      display:flex;
      align-items:baseline;
      justify-content:space-between;
      gap:12px;
    }
    .phase-header h2{ margin:0; }
  </style>
</head>

<body>
  <!-- ===== Site Navbar (EXACT copy from your example) ===== -->
  <nav class="container">
    <div class="logo"><a href="/#home" style="text-decoration:none; color:inherit">Safi Ahmed</a></div>
    <ul class="nav-links">
      <li><a href="/#home">Home</a></li>
      <li><a href="/#education">Education</a></li>
      <li><a href="/#experience">Experience</a></li>
      <li><a href="/#publications">Publications</a></li>
      <li><a href="/projects">Projects</a></li>
      <li><a href="/#contact">Contact</a></li>
    </ul>
    <div class="menu-toggle" aria-label="Toggle menu">‚ò∞</div>
  </nav>

  <!-- ===== HERO ===== -->
  <section class="container section-card" id="hero">
    <a class="pill" href="/projects">‚Üê Back to Projects</a>
    <h1 style="margin:10px 0 6px">Real-World ML on Wearable Sensor Signals</h1>
    <p class="small" style="margin:6px 0 0">
      From raw oscilloscope CSVs to trained models: segmentation ‚Üí dataset build ‚Üí models (CNN/ResNet1D/Transformer/TCN) ‚Üí evaluation & export.
    </p>
    <div class="tags" style="margin-top:10px">
      <span class="tag">Signal Processing</span><span class="tag">Time-Series</span><span class="tag">Segmentation</span>
      <span class="tag">Model Zoo</span><span class="tag">Confusion Matrices</span>
    </div>

    <div class="actions" style="margin-top:12px;">
      <a class="btn-link" href="#phase2">Phase 2</a>
      <a class="btn-link" href="#phase1">Phase 1</a>
      <a class="btn-link" href="#run">Reproduce</a>
      <a class="btn-link" href="https://github.com/sappiahmed/Sliding">GitHub</a>
    </div>
  </section>

  <section class="container section-card" id="phase2">
  <div class="phase-header">
    <h2>Phase 2</h2>
    <span class="small">Real-time ML inference + ESP32 data collection + training pipeline</span>
  </div>

  <div class="grid two" style="margin-top:14px;">

    <!-- LEFT: two figures -->
    <div>
      <h3 style="margin-top:0">Realtime pipeline snapshots</h3>

      <p class="small" style="margin:8px 0 6px;">Sliding signal (live stream / windowed input)</p>
      <img src="phase2_signal.png" alt="Phase 2: sliding signal">

      <p class="small" style="margin:14px 0 6px;">Training curves (epoch vs loss and accuracy)</p>
      <img src="phase2_curves.png" alt="Phase 2: epoch vs loss and accuracy">

      <p class="small" style="margin-top:12px;">
        FINAL TEST ACCURACY: 95.24%
      </p>
    </div>

    <!-- RIGHT: youtube + confusion matrix below -->
    <div>
      <h3 style="margin-top:0">Real-time inference demo</h3>

      <div class="video" style="margin-top:10px;">
        <div class="ratio">
          <iframe
            src="https://www.youtube.com/embed/ztL5reCQoxg"
            title="Real-time inference demo"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
            allowfullscreen>
          </iframe>
        </div>
      </div>

      <p class="small" style="margin:14px 0 6px;">Confusion matrix (test set)</p>
      <img src="phase2_cm.png" alt="Phase 2: confusion matrix">

      <div class="callout" style="margin-top:14px">
        <strong>What‚Äôs new in Phase 2</strong>
        <ul style="margin-top:8px;">
          <li>ESP32-based data acquisition & structured dataset logging</li>
          <li>PC-side real-time preprocessing ‚Üí inference loop</li>
          <li>Training + evaluation loop tied to the same label set used in deployment</li>
        </ul>
      </div>
    </div>

  </div>
</section>


  <!-- ===== OVERVIEW (two columns inside one card) ===== -->
  <section class="container section-card" id="overview">
    <h2>Phase 1</h2>
    <div class="grid two">
      <div>
        <p>
          This project skips pre-packaged datasets. It ingests <em>raw</em> sliding signals (CSV: time, voltage),
          detects peaks, anchors to a local pre-peak, and slices fixed windows so models learn
          shape/dynamics instead of timing jitter.
        </p>
        <ul>
          <li>CSV per motion & per subject</li>
          <li>Pre-peak anchor for alignment</li>
          <li>Unified dataset with <code>user</code> & <code>label</code></li>
        </ul>
      </div>
      <div>
        <h3 style="margin-top:0">Example signals</h3>
        <img src="w1.png" alt="Raw signals ‚Üí peaks ‚Üí aligned segments">
        <p class="small">Raw trace, detected peaks, aligned segments per subject.</p>
      </div>
    </div>
  </section>

  <!-- ===== PIPELINE ===== -->
  <section class="container section-card" id="pipeline">
    <h2>Pipeline</h2>
    <ol>
      <li><strong>Segmentation</strong> ‚Äî parse each CSV, detect main peaks, find a local pre-peak anchor, then slice a fixed window.</li>
      <li><strong>Dataset build</strong> ‚Äî pad to max length, stack into a single CSV; add <code>user</code> and <code>label</code>.</li>
      <li><strong>Config</strong> ‚Äî epochs, LR, model name, normalize/augment toggles.</li>
      <li><strong>Models</strong> ‚Äî CNN, ResNet1D, Inception-style, CNN-LSTM, Transformer, TCN (via factory).</li>
      <li><strong>Training</strong> ‚Äî Adam + StepLR; save history and final test metrics.</li>
      <li><strong>Evaluation</strong> ‚Äî accuracy, per-class metrics, confusion matrices; auto-export combined figure.</li>
    </ol>
    <div class="callout" style="margin-top:14px">
      <strong>Why pre-peak anchoring?</strong> It reduces temporal jitter across repetitions so models generalize to motion dynamics instead of misalignment artifacts.
    </div>
  </section>

  <!-- ===== RESULTS (two columns inside one card) ===== -->
  <section class="container section-card" id="results">
    <h2>Results</h2>
    <div class="grid two">
      <div>
        <h3 style="margin-top:0">Per-subject distributions</h3>
        <p>Representative overlays of aligned segments for each subject.</p>
        <img src="w2.png" alt="Per-subject overlays montage">
      </div>
      <div>
        <h3 style="margin-top:0">Auto-exported summary</h3>
        <ul>
          <li>Train/Val curves across epochs</li>
          <li>Confusion matrices (counts + row-normalized)</li>
          <li>Run metadata (model, batch size, LR, normalize/augment, dataset path)</li>
        </ul>
        <img src="w3.png" alt="Run summary">
        <p class="small">History CSV is saved alongside the PNG for reproducibility.</p>
      </div>
    </div>
  </section>

  <!-- ===== HOW TO RUN ===== -->
  <section class="container section-card" id="run">
    <h2>Reproduce this</h2>
    <p>Minimal steps to go from raw CSVs to trained models and figures:</p>
    <div class="code">
<pre># 1) Point config to your raw folder & dataset CSV (auto-builds if missing)
raw_root_dir = "D:/path/to/Microcone"
dataset_path = "microcone_segments_dataset.csv"

# 2) Select a model and training knobs
model_name   = "tcn"      # "cnn_tuned", "resnet1d", "ts_transformer", "tcn"
num_epochs   = 30
lr           = 1e-3
normalize    = True
augment      = True       # light Gaussian noise

# 3) Train and export results
python scripts/run_train.py   # creates /results/<*.png + history.csv></pre>
    </div>
    <p class="small" style="margin-top:10px">Splits: Train/Val/Test = 70/15/15 with stratification.</p>
  </section>

  <!-- ===== FILE TREE ===== -->
  <section class="container section-card" id="files">
    <h2>Project layout</h2>
    <div class="code">
<pre>ml_project/
  config.py         # centralized training knobs (paths, lr, epochs, model) 
  data.py           # segmentation + dataset build + dataloaders            
  models.py         # model zoo & registry                                  
  train.py          # train loop + final test + export                      
  evaluate.py       # metrics + 2√ó2 summary figure                          
  utils.py          # seed, epoch runner, full evaluate                      
  scripts/
    run_train.py    # single run from config
    run_grid_2.py   # multi-model, multi-param grid</pre>
    </div>
  </section>

  <!-- ===== Next ===== -->
<!-- ===== Next (Technical Improvements) ===== -->
<section class="container section-card" id="next">
  <h2>What‚Äôs Next</h2>

  <p class="small">
    Moving from real-time classification to <strong>adaptive, closed-loop interaction</strong>.
  </p>

  <div class="grid two" style="margin-top:16px;">

    <!-- LEFT COLUMN -->
    <div>
      <h3 style="margin-top:0">üöÄ Applications</h3>
      <ul>
        <li>
          <strong>Robotic manipulation</strong><br>
          Gesture-driven control of robotic arms and mobile robots.
        </li>
        <li>
          <strong>Vehicle & assistive interfaces</strong><br>
          Sensor-based steering, throttle, or mode selection.
        </li>
        <li>
          <strong>Human‚Äìcomputer interaction</strong><br>
          Real-time control for games and virtual environments.
        </li>
      </ul>
    </div>

    <!-- RIGHT COLUMN -->
    <div>
      <h3 style="margin-top:0">üß† Learning & Adaptation</h3>
      <ul>
        <li>
          <strong>Human-in-the-loop learning</strong><br>
          User confirms or corrects predictions during inference.
        </li>
        <li>
          <strong>Online model improvement</strong><br>
          Verified samples are logged and used to refine the model.
        </li>
        <li>
          <strong>Transfer & continual learning</strong><br>
          Pretrained base model adapts to new users with minimal data.
        </li>
      </ul>
    </div>

  </div>

  <div class="callout" style="margin-top:18px">
    <strong>Vision =></strong> A wearable-sensor system that learns from the user in real time,
    improving accuracy and control the more it is used.
  </div>
</section>



  <!-- ===== FOOTER ===== -->
  <section class="container" style="padding:0; margin-bottom:28px">
    <p class="small">¬© <span id="year"></span> Safi Ahmed ‚Ä¢ <a href="/#contact">Contact</a></p>
  </section>

  <script>
    document.getElementById("year").textContent = new Date().getFullYear();
  </script>
</body>
</html>
